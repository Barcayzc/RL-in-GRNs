{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86077d34",
   "metadata": {},
   "source": [
    "**Q_network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b022f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (32, 100)\n",
      "Output shape: (32, 101)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"q_network\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"q_network\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ q_values (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,565</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_1 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │         \u001b[38;5;34m6,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ hidden_2 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ q_values (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m101\u001b[0m)              │         \u001b[38;5;34m6,565\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,189</span> (67.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,189\u001b[0m (67.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,189</span> (67.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,189\u001b[0m (67.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class QNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A simple fully-connected Q-network for DQN/DDQN.\n",
    "\n",
    "    Architecture:\n",
    "      Input:  state vector of dimension `input_dim`\n",
    "      Hidden layers: sequence of Dense(hidden_units[i]) with ReLU\n",
    "      Output layer: Dense(output_dim) producing Q-values (linear activation)\n",
    "\n",
    "    Example usage:\n",
    "        model = QNetwork(input_dim=100, output_dim=101)\n",
    "        q_values = model(tf.random.uniform((batch_size, 100)))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        hidden_units: list[int] = [64, 64],\n",
    "        name: str = \"q_network\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the QNetwork.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimension of the input state vector.\n",
    "            output_dim: Number of actions (size of the Q-value output).\n",
    "            hidden_units: List of integers, the number of units in each hidden layer.\n",
    "            name: Optional name for the Keras model.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        # Create hidden layers\n",
    "        self.hidden_layers = []\n",
    "        for i, units in enumerate(hidden_units):\n",
    "            self.hidden_layers.append(\n",
    "                tf.keras.layers.Dense(\n",
    "                    units,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_uniform\",\n",
    "                    name=f\"hidden_{i+1}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Output layer (linear activation for Q-values)\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            output_dim,\n",
    "            activation=None,\n",
    "            kernel_initializer=\"he_uniform\",\n",
    "            name=\"q_values\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: computes Q-values for each action.\n",
    "\n",
    "        Args:\n",
    "            inputs: Tensor of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            q_values: Tensor of shape (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        x = tf.cast(inputs, tf.float32)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "        q_values = self.output_layer(x)\n",
    "        return q_values\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        Optional helper: builds the model by passing a dummy input.\n",
    "        Useful to display the model summary before training.\n",
    "        \"\"\"\n",
    "        dummy = tf.keras.Input(shape=(None, ), dtype=tf.float32)\n",
    "        self.call(dummy)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick smoke test\n",
    "    input_dim = 100\n",
    "    output_dim = 101\n",
    "    batch_size = 32\n",
    "\n",
    "    model = QNetwork(input_dim=input_dim, output_dim=output_dim)\n",
    "    # Build the model by calling it once\n",
    "    dummy_input = tf.random.uniform((batch_size, input_dim))\n",
    "    dummy_q = model(dummy_input)\n",
    "\n",
    "    print(\"Input shape:\", dummy_input.shape)\n",
    "    print(\"Output shape:\", dummy_q.shape)\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873048a",
   "metadata": {},
   "source": [
    "**Prioritized Replay Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03ef4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "Transition = Tuple[Any, Any, float, Any, bool]\n",
    "EPSILON = 1e-6  # small constant to avoid zero priority\n",
    "\n",
    "\n",
    "class SumSegmentTree:\n",
    "    \"\"\"Binary indexed segment tree supporting sum queries and prefix‐sum indexing.\"\"\"\n",
    "    def __init__(self, capacity: int):\n",
    "        # Next power of two for capacity\n",
    "        self._n = 1\n",
    "        while self._n < capacity:\n",
    "            self._n <<= 1\n",
    "        self._size = capacity\n",
    "        # Tree array: [1 .. 2*n), 1-based indexing at root=1\n",
    "        self._tree = np.zeros(2 * self._n, dtype=np.float32)\n",
    "\n",
    "    def update(self, idx: int, value: float):\n",
    "        \"\"\"Set value at leaf idx, then update internal nodes.\"\"\"\n",
    "        tree_idx = idx + self._n\n",
    "        self._tree[tree_idx] = value\n",
    "        # Walk up and update parents\n",
    "        parent = tree_idx >> 1\n",
    "        while parent >= 1:\n",
    "            self._tree[parent] = self._tree[2*parent] + self._tree[2*parent + 1]\n",
    "            parent >>= 1\n",
    "\n",
    "    def sum_total(self) -> float:\n",
    "        \"\"\"Returns sum over all leaf values.\"\"\"\n",
    "        return float(self._tree[1])\n",
    "\n",
    "    def find_prefixsum_idx(self, prefix: float) -> int:\n",
    "        \"\"\"\n",
    "        Find highest idx such that cumulative sum up to idx >= prefix.\n",
    "        Returns a leaf index in [0, size).\n",
    "        \"\"\"\n",
    "        idx = 1\n",
    "        while idx < self._n:  # while not at leaf\n",
    "            left = 2 * idx\n",
    "            if self._tree[left] >= prefix:\n",
    "                idx = left\n",
    "            else:\n",
    "                prefix -= self._tree[left]\n",
    "                idx = left + 1\n",
    "        return idx - self._n\n",
    "\n",
    "\n",
    "class MinSegmentTree:\n",
    "    \"\"\"Similar to SumSegmentTree but supports range minimum query over priorities.\"\"\"\n",
    "    def __init__(self, capacity: int):\n",
    "        # Next power of two for capacity\n",
    "        self._n = 1\n",
    "        while self._n < capacity:\n",
    "            self._n <<= 1\n",
    "        self._size = capacity\n",
    "        # Initialize with +inf so unused leaves don't interfere\n",
    "        self._tree = np.full(2 * self._n, float('inf'), dtype=np.float32)\n",
    "\n",
    "    def update(self, idx: int, value: float):\n",
    "        \"\"\"Set value at leaf idx, then update internal nodes with min.\"\"\"\n",
    "        tree_idx = idx + self._n\n",
    "        self._tree[tree_idx] = value\n",
    "        parent = tree_idx >> 1\n",
    "        while parent >= 1:\n",
    "            self._tree[parent] = min(self._tree[2*parent], self._tree[2*parent + 1])\n",
    "            parent >>= 1\n",
    "\n",
    "    def min(self) -> float:\n",
    "        \"\"\"Returns minimum over all leaf values.\"\"\"\n",
    "        return float(self._tree[1])\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"\n",
    "    Standalone Prioritized Experience Replay Buffer.\n",
    "\n",
    "    Stores transitions with priorities, supports sampling by priority,\n",
    "    and updating priorities. Internally uses a SumSegmentTree for\n",
    "    proportional sampling and a MinSegmentTree for retrieving the\n",
    "    minimum priority (for importance‐sampling weight normalization).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int, alpha: float = 0.6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity: Maximum number of transitions to store.\n",
    "            alpha: Priority exponent (0 = uniform sampling, 1 = full prioritization).\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Segment trees\n",
    "        self._sum_tree = SumSegmentTree(capacity)\n",
    "        self._min_tree = MinSegmentTree(capacity)\n",
    "\n",
    "        # Experience storage\n",
    "        self._data: List[Transition] = [None] * capacity\n",
    "        self._next_idx = 0\n",
    "        self._size = 0\n",
    "\n",
    "        # Track maximal priority for new transitions\n",
    "        self._max_priority = 1.0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._size\n",
    "    size = __len__\n",
    "\n",
    "    def store(self, transition: Transition):\n",
    "        \"\"\"\n",
    "        Adds a new transition to the buffer with maximal priority.\n",
    "\n",
    "        Args:\n",
    "            transition: A tuple (state, action, reward, next_state, done).\n",
    "        \"\"\"\n",
    "        idx = self._next_idx\n",
    "        self._data[idx] = transition\n",
    "\n",
    "        # Assign max priority to new transition\n",
    "        priority = self._max_priority ** self.alpha\n",
    "        self._sum_tree.update(idx, priority)\n",
    "        self._min_tree.update(idx, priority)\n",
    "\n",
    "        # Advance pointer\n",
    "        self._next_idx = (self._next_idx + 1) % self.capacity\n",
    "        self._size = min(self._size + 1, self.capacity)\n",
    "    add=store\n",
    "    \n",
    "    def sample(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        beta: float = 0.4\n",
    "    ) -> Tuple[List[Transition], List[int], np.ndarray]:\n",
    "        \"\"\"\n",
    "        Samples a batch of transitions with probabilities proportional to priority.\n",
    "        Returns transitions, their indices, and importance‐sampling weights.\n",
    "\n",
    "        Args:\n",
    "            batch_size: Number of transitions to sample.\n",
    "            beta: Importance-sampling exponent (0 = no correction, 1 = full correction).\n",
    "\n",
    "        Returns:\n",
    "            transitions: List of sampled transitions.\n",
    "            indices: List of indices in the buffer.\n",
    "            weights: Array of shape (batch_size,) of IS weights in [0,1].\n",
    "        \"\"\"\n",
    "        assert self._size > 0, \"Cannot sample from an empty buffer\"\n",
    "\n",
    "        # Total priority mass\n",
    "        total_sum = self._sum_tree.sum_total()\n",
    "        segment = total_sum / batch_size\n",
    "\n",
    "        transitions = []\n",
    "        indices = []\n",
    "        weights = np.empty(batch_size, dtype=np.float32)\n",
    "\n",
    "        # Minimum probability for weight normalization\n",
    "        min_prob = self._min_tree.min() / total_sum\n",
    "        max_weight = (min_prob * self._size) ** (-beta)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx = self._sum_tree.find_prefixsum_idx(s)\n",
    "\n",
    "            transitions.append(self._data[idx])\n",
    "            indices.append(idx)\n",
    "\n",
    "            # Compute importance-sampling weight\n",
    "            p_i = self._sum_tree._tree[idx + self._sum_tree._n] / total_sum\n",
    "            w = (p_i * self._size) ** (-beta)\n",
    "            weights[i] = w / max_weight  # normalize to [0, 1]\n",
    "\n",
    "        return transitions, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices: List[int], priorities: List[float]):\n",
    "        \"\"\"\n",
    "        Updates the priorities of sampled transitions.\n",
    "\n",
    "        Args:\n",
    "            indices: List of buffer indices for the transitions.\n",
    "            priorities: List of new priority values (e.g. absolute TD errors).\n",
    "        \"\"\"\n",
    "        for idx, p in zip(indices, priorities):\n",
    "            # Add a small epsilon and apply alpha exponent\n",
    "            p_adjusted = (abs(p) + EPSILON) ** self.alpha\n",
    "            self._sum_tree.update(idx, p_adjusted)\n",
    "            self._min_tree.update(idx, p_adjusted)\n",
    "            # Track max raw priority for new inserts\n",
    "            self._max_priority = max(self._max_priority, abs(p) + EPSILON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a633b3",
   "metadata": {},
   "source": [
    "buffer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4636b74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer size (should be 8): 8\n",
      "\n",
      "Sampled transitions:\n",
      " idx= 1  tr=(array([1]), 1, 1.0, array([2]), False)  w=1.0000\n",
      " idx= 3  tr=(array([3]), 3, 3.0, array([4]), False)  w=1.0000\n",
      " idx= 5  tr=(array([5]), 5, 5.0, array([6]), False)  w=1.0000\n",
      " idx= 7  tr=(array([7]), 7, 7.0, array([8]), False)  w=1.0000\n",
      " idx= 1  count=437\n",
      " idx= 3  count=724\n",
      " idx= 5  count=869\n",
      " idx= 7  count=1132\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1) Create a tiny buffer\n",
    "buf = PrioritizedReplayBuffer(capacity=8, alpha=0.6)\n",
    "\n",
    "# 2) Push in some “dummy” transitions\n",
    "for i in range(8):\n",
    "    # (state, action, reward, next_state, done)\n",
    "    tr = (np.array([i]), i, float(i), np.array([i+1]), False)\n",
    "    buf.store(tr)\n",
    "\n",
    "print(\"Buffer size (should be 8):\", len(buf))\n",
    "\n",
    "# 3) Sample a batch\n",
    "batch, idxs, weights = buf.sample(batch_size=4, beta=0.4)\n",
    "print(\"\\nSampled transitions:\")\n",
    "for t, idx, w in zip(batch, idxs, weights):\n",
    "    print(f\" idx={idx:2d}  tr={t}  w={w:.4f}\")\n",
    "\n",
    "# 4) Check shapes\n",
    "assert len(batch) == 4\n",
    "assert len(idxs)  == 4\n",
    "assert weights.shape == (4,)\n",
    "\n",
    "# 5) Artificially “update” their priorities (e.g. new TD‐errors = [1,2,3,4])\n",
    "new_prios = [1.0, 2.0, 3.0, 4.0]\n",
    "buf.update_priorities(idxs, new_prios)\n",
    "\n",
    "# 6) Re‐sample and see if the indices with higher prios appear more often\n",
    "counts = defaultdict(int)           # will auto‐zero missing keys\n",
    "for _ in range(5000):\n",
    "    _, (idx,), _ = buf.sample(batch_size=1, beta=1.0)\n",
    "    counts[idx] += 1\n",
    "# Now inspect just the ones we care about:\n",
    "for idx in idxs:\n",
    "    print(f\" idx={idx:2d}  count={counts[idx]}\")\n",
    "\n",
    "\n",
    "# If everything is correctly:\n",
    "#  - No exceptions should be raised\n",
    "#  - The ‘counts’ for higher‐priority indices (those we gave larger new_prios)\n",
    "#    should be noticeably larger than for the smaller‐priority ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5076ac",
   "metadata": {},
   "source": [
    "**DDQN + PER Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f52e3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from q_network import QNetwork\n",
    "from per_buffer import PrioritizedReplayBuffer\n",
    "\n",
    "\n",
    "class DDQNPERAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        gamma=0.99,\n",
    "        learning_rate=1e-3,\n",
    "        epsilon=1.0,\n",
    "        epsilon_min=0.1,\n",
    "        epsilon_decay=0.995,\n",
    "        batch_size=64,\n",
    "        memory_capacity=100_000,\n",
    "        alpha=0.6,\n",
    "        beta=0.4,\n",
    "        beta_increment=1e-6,\n",
    "        target_update_freq=1000,\n",
    "    ):\n",
    "        # Dimensions\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.beta = beta\n",
    "        self.beta_increment = beta_increment\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "        # Replay memory with Prioritized Experience Replay\n",
    "        self.memory = PrioritizedReplayBuffer(capacity=memory_capacity, alpha=alpha)\n",
    "\n",
    "        # Q-Networks: online & target\n",
    "        self.q_network = QNetwork(self.state_dim, self.action_dim)\n",
    "        self.target_q_network = QNetwork(self.state_dim, self.action_dim)\n",
    "        # Initialize target weights\n",
    "        self.target_q_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "        # Optimizer and loss\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "        self.loss_fn = tf.keras.losses.Huber()\n",
    "\n",
    "        # Training step counter\n",
    "        self.train_step_count = 0\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        # Greedy action from Q-network\n",
    "        state_tensor = tf.convert_to_tensor(state[None, :], dtype=tf.float32)\n",
    "        q_values = self.q_network(state_tensor)\n",
    "        return int(tf.argmax(q_values[0]).numpy())\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Adds a transition to the replay buffer.\n",
    "        \"\"\"\n",
    "        transition = (state, action, reward, next_state, done)\n",
    "        self.memory.add(transition)\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Samples a batch from memory and updates the Q-network.\n",
    "        \"\"\"\n",
    "        # Do not train until enough samples\n",
    "        if self.memory.size() < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample from PER memory\n",
    "        (states, actions, rewards, next_states, dones,\n",
    "         weights, indices) = self.memory.sample(self.batch_size, beta=self.beta)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states_tf = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        next_states_tf = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        actions_tf = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        rewards_tf = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        dones_tf = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "        weights_tf = tf.convert_to_tensor(weights, dtype=tf.float32)\n",
    "\n",
    "        # Double DQN target computation\n",
    "        # 1) Action selection with online network\n",
    "        q_next_online = self.q_network(next_states_tf)\n",
    "        best_actions = tf.argmax(q_next_online, axis=1)\n",
    "        # 2) Q-value evaluation with target network\n",
    "        q_next_target = self.target_q_network(next_states_tf)\n",
    "        batch_indices = tf.range(self.batch_size, dtype=tf.int64)\n",
    "        target_q_values = tf.gather_nd(q_next_target,\n",
    "                                        tf.stack([batch_indices, best_actions], axis=1))\n",
    "        # Compute TD targets: r + gamma * Q_target(s', argmax Q_online)\n",
    "        targets = rewards_tf + self.gamma * target_q_values * (1 - dones_tf)\n",
    "\n",
    "        # Train online Q-network\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.q_network(states_tf)\n",
    "            # Select Q-values for taken actions\n",
    "            action_mask = tf.one_hot(actions_tf, self.action_dim, dtype=tf.float32)\n",
    "            q_selected = tf.reduce_sum(q_values * action_mask, axis=1)\n",
    "\n",
    "            # Compute Huber loss per sample\n",
    "            loss_unweighted = self.loss_fn(targets, q_selected)\n",
    "            # Apply importance-sampling weights\n",
    "            loss = tf.reduce_mean(weights_tf * loss_unweighted)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "        # Update priorities in replay buffer using absolute TD errors\n",
    "        td_errors = tf.abs(targets - q_selected).numpy() + 1e-6\n",
    "        self.memory.update_priorities(indices, td_errors)\n",
    "\n",
    "        # Increment training step counter\n",
    "        self.train_step_count += 1\n",
    "\n",
    "        # Periodic target network update\n",
    "        if (self.train_step_count % self.target_update_freq) == 0:\n",
    "            self.update_target_network()\n",
    "\n",
    "        # Anneal epsilon and beta\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Copies online network weights to target network.\n",
    "        \"\"\"\n",
    "        self.target_q_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Saves the online Q-network to the given path.\n",
    "        \"\"\"\n",
    "        self.q_network.save(path)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Loads weights into the online Q-network and syncs the target network.\n",
    "        \"\"\"\n",
    "        self.q_network = tf.keras.models.load_model(path)\n",
    "        self.target_q_network.set_weights(self.q_network.get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234d599",
   "metadata": {},
   "source": [
    "**Agent test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3583ed6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_prepared/gene_names_safe.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mddqn_per_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DDQNPERAgent\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpbn_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_env\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msmoke_test\u001b[39m():\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# 1) Build the env\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     env \u001b[38;5;241m=\u001b[39m make_env(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Spring2025/CSCI1470/Final_Project/RL-in-GRNs/pbn_env/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpbn_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_env\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake_env\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Spring2025/CSCI1470/Final_Project/RL-in-GRNs/pbn_env/pbn_env.py:26\u001b[0m\n\u001b[1;32m     20\u001b[0m GENE_NAMES_PATH   \u001b[38;5;241m=\u001b[39m DATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgene_names_safe.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# One-time loading of gene names & logic functions\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# (executed once when the module is first imported)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# ─────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGENE_NAMES_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     27\u001b[0m     NODE_NAMES \u001b[38;5;241m=\u001b[39m [ln\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m ln \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;28;01mif\u001b[39;00m ln\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(LOGIC_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_prepared/gene_names_safe.txt'"
     ]
    }
   ],
   "source": [
    "# scripts/smoke_test_agent.py\n",
    "\n",
    "import os, sys\n",
    "\n",
    "try:\n",
    "    here = os.path.dirname(__file__)\n",
    "except NameError:\n",
    "    here = os.getcwd()\n",
    "\n",
    "root = os.path.abspath(os.path.join(here, os.pardir))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from ddqn_per_agent import DDQNPERAgent\n",
    "from pbn_env import make_env\n",
    "\n",
    "\n",
    "def smoke_test():\n",
    "    # 1) Build the env\n",
    "    env = make_env(seed=0)\n",
    "\n",
    "    # 2) Inspect spaces\n",
    "    state_dim  = env.observation_space.n  # e.g. 100\n",
    "    action_dim = env.action_space.n       # e.g. 101\n",
    "    print(f\"State dim = {state_dim}, Action dim = {action_dim}\")\n",
    "\n",
    "    # 3) Build the agent\n",
    "    agent = DDQNPERAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        batch_size=32,\n",
    "        memory_capacity=1000,\n",
    "        target_update_freq=50,\n",
    "    )\n",
    "\n",
    "    # 4) Warm up: one env reset\n",
    "    state, _ = env.reset()\n",
    "    print(\"Initial state (first 10 bits):\", state[:10].astype(int))\n",
    "\n",
    "    # 5) One episode of random+train steps\n",
    "    for step in range(20):\n",
    "        # a) Action (epsilon-greedy)\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # b) Env step\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # c) Store and train\n",
    "        agent.store_transition(state, action, reward, next_state, terminated)\n",
    "        agent.train_step()\n",
    "\n",
    "        # d) Next state (with auto-reset on done)\n",
    "        if terminated or truncated:\n",
    "            state, _ = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "        # e) Print a line or two\n",
    "        print(f\"Step {step:2d}: a={action:3d}, r={reward:+.1f}, eps={agent.epsilon:.3f}\")\n",
    "\n",
    "    # 6) Final sanity checks\n",
    "    print(\"Final epsilon:\", agent.epsilon)\n",
    "    print(\"Replay buffer size:\", agent.memory.size())\n",
    "\n",
    "    print(\"✅ Smoke test completed without errors.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    smoke_test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
